{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGCluster dRep\n",
    "Will cluster the clusters based on Average Nucleotide Identity... and choose the best one based on length.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import SeqIO\n",
    "import subprocess\n",
    "import drep\n",
    "from drep import d_analyze\n",
    "import pandas as pd\n",
    "from shutil import copyfile\n",
    "from itertools import product \n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data dir paths \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/AS_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard-coded Antismash directories\n",
    "\n",
    "Pat_all_AS = data_dir + '/MH-Pat-all_Masurca.AS/'\n",
    "Pat_sr_AS = data_dir + '/MH-Pat-sr_spades.AS/'\n",
    "S1_AS = data_dir + '/MH-s1_L2.2k.fa.AS/'\n",
    "S2_AS = data_dir + '/MH-s2_L2.2k.fa.AS/'\n",
    "S3_AS = data_dir + '/MH-s3_L2.2k.fa.AS/'\n",
    "S5_AS = data_dir + '/MH-s5_L2.2k.fa.AS/'\n",
    "\n",
    "\n",
    "AS_DIR_list = [S1_AS, S2_AS, S3_AS, S5_AS, Pat_all_AS, Pat_sr_AS]\n",
    "\n",
    "## Directory of fastas for the bins\n",
    "bins_dir = '/home/matt/Desktop/final_bins/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will get all the individual gene clusters, in gbk, from and antismash output dir and return an output dir (BGCs_fastas) of fastas, \n",
    "# one fasta file for each gene cluster with a good naming convention.\n",
    "\n",
    "\n",
    "def BGC_gbks_to_fastas(AS_DIR):\n",
    "\n",
    "    # The list of individual BGC gbks in an AS output dir, excluding 'final'\n",
    "    gbks = []\n",
    "    \n",
    "    ##Make a results dir\n",
    "    if not os.path.isdir('data/BGCs_fastas'):\n",
    "        os.makedirs('data/BGCs_fastas')\n",
    "        \n",
    "    \n",
    "    ## Populate gbks list with the paths to all the appropiate gbk files in the dir\n",
    "    for file in os.listdir(AS_DIR):\n",
    "        if file.split('.')[-1] == 'gbk':\n",
    "            if file.split('.')[-2] != 'final':\n",
    "                    gbks.append(AS_DIR + file)\n",
    "\n",
    "\n",
    "    ## For each BGC gbk, write the full Nucleotide sequence to a multi-fasta file.   \n",
    "\n",
    "    seq_records = []\n",
    "\n",
    "    for gbk in gbks:\n",
    "\n",
    "        ## Get a useable name for each BGC gbk in the list, format: assembly-name_clusrer-number ie 'PAT_cluster001'\n",
    "        ## The will become the headers in the fasta file\n",
    "        \n",
    "        gbk_name_split = gbk.split('.') \n",
    "\n",
    "        assem_name = gbk_name_split[0].split('/')[-1]\n",
    "        clust_name = gbk_name_split[-2]\n",
    "\n",
    "        header = assem_name + '_' + clust_name\n",
    "\n",
    "\n",
    "        ## Get seq record for the BGC\n",
    "        records = SeqIO.parse(gbk, \"genbank\")\n",
    "        for rec in records:\n",
    "            rec.id = header ## changer the header for the record to the one generated above\n",
    "\n",
    "            seq_records.append(rec)\n",
    "\n",
    "\n",
    "    ## Write the seq_records to a file\n",
    "    ## Make a name for the output dir \n",
    "    assembly_name = seq_records[0].id.split('_')[0]\n",
    "    \n",
    "    ## Make directory for results\n",
    "    \n",
    "    os.mkdir('data/BGCs_fastas/'+assembly_name)\n",
    "    \n",
    "    #get current dir \n",
    "\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    \n",
    "    \n",
    "    ## Write out the results to a named directory inside the output directory\n",
    "    for rec in seq_records:\n",
    "        id = rec.id\n",
    "        seq = rec.seq\n",
    "        id_file = open(cwd+'/'+'data/BGCs_fastas/' + assembly_name +'/'+id+'.fna', \"w\")\n",
    "        id_file.write(\">\"+str(id)+\"\\n\"+str(seq))\n",
    "        id_file.close()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Already done\n",
    "# for AS_DIR in AS_DIR_list:\n",
    "#     BGC_gbks_to_fastas(AS_DIR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Walk through the dirs and make a list of all fna's for the dRep run. This will get all *.fna in the \n",
    "## cwd recursively, so becareful that there are no other .fna files lying around.\n",
    "## shoud be 430 files in total  \n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "fnas_list = []\n",
    "for root, dirs, files in os.walk('data/BGCs_fastas'):\n",
    "\n",
    "    for name in files:\n",
    "        if name.endswith('.fna'):\n",
    "            fnas_list.append(os.path.join(cwd,root,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the dRep clustering with the python API\n",
    "\n",
    "\n",
    "#drep.d_cluster.d_cluster_wrapper('./assemblies_drep_wd_ANImf_25', P_ani = 0.85, S_ani =0.9, S_algorithm = 'ANImf', genomes = fnas_list, COV_THRESH = 0.25)\n",
    "\n",
    "# plot the outputs\n",
    "#drep.d_analyze.d_analyze_wrapper('./assemblies_drep_wd_ANImf_25', plots = ['1','2','3','4'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the longest BGC from each dRep cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in the clustering data table from dRep\n",
    "\n",
    "clus_df = pd.read_csv('assemblies_drep_wd_ANImf_25/data_tables/Cdb.csv')\n",
    "\n",
    "\n",
    "## Make a df of cluster lengths and paths\n",
    "\n",
    "genomes = [] \n",
    "lengths = []\n",
    "paths = []\n",
    "\n",
    "for fna in fnas_list:\n",
    "    record = SeqIO.parse(fna, 'fasta')\n",
    "    \n",
    "    paths.append(fna)\n",
    "    for rec in record:\n",
    "        name = rec.id\n",
    "        BCG_length = len(rec.seq)\n",
    "        \n",
    "    genomes.append(name+'.fna')\n",
    "    lengths.append(BCG_length)\n",
    "    \n",
    "\n",
    "BGC_df = pd.DataFrame( {'genome':genomes, 'length':lengths, 'path':paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge dataframes to add lenght and path to clus_df\n",
    "\n",
    "clus_df = clus_df.merge(BGC_df, on='genome').drop(['threshold','cluster_method','comparison_algorithm'], axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genome</th>\n",
       "      <th>secondary_cluster</th>\n",
       "      <th>primary_cluster</th>\n",
       "      <th>length</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MH-s3_L2_cluster089.fna</td>\n",
       "      <td>1_1</td>\n",
       "      <td>1</td>\n",
       "      <td>3231</td>\n",
       "      <td>/home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MH-s5_L2_cluster047.fna</td>\n",
       "      <td>1_2</td>\n",
       "      <td>1</td>\n",
       "      <td>2438</td>\n",
       "      <td>/home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MH-s3_L2_cluster083.fna</td>\n",
       "      <td>2_1</td>\n",
       "      <td>2</td>\n",
       "      <td>3836</td>\n",
       "      <td>/home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MH-s5_L2_cluster024.fna</td>\n",
       "      <td>2_1</td>\n",
       "      <td>2</td>\n",
       "      <td>27169</td>\n",
       "      <td>/home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MH-s2_L2_cluster096.fna</td>\n",
       "      <td>3_1</td>\n",
       "      <td>3</td>\n",
       "      <td>3320</td>\n",
       "      <td>/home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    genome secondary_cluster  primary_cluster  length  \\\n",
       "0  MH-s3_L2_cluster089.fna               1_1                1    3231   \n",
       "1  MH-s5_L2_cluster047.fna               1_2                1    2438   \n",
       "2  MH-s3_L2_cluster083.fna               2_1                2    3836   \n",
       "3  MH-s5_L2_cluster024.fna               2_1                2   27169   \n",
       "4  MH-s2_L2_cluster096.fna               3_1                3    3320   \n",
       "\n",
       "                                                path  \n",
       "0  /home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_f...  \n",
       "1  /home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_f...  \n",
       "2  /home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_f...  \n",
       "3  /home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_f...  \n",
       "4  /home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_f...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the origianl contig id to the data frame\n",
    "\n",
    "\n",
    "## The antismash dirs mapped to assembly names \n",
    "assembly_names = {'MH-Pat-all':Pat_all_AS,\n",
    "                  'MH-Pat-sr':Pat_sr_AS,\n",
    "                  'MH-s1': S1_AS,\n",
    "                  'MH-s2': S2_AS,\n",
    "                  'MH-s3': S3_AS,\n",
    "                  'MH-s5': S5_AS}\n",
    "\n",
    "\n",
    "\n",
    "## Load in the 'geneclusters' file for each assembly antismash dir as a pandas df and overwrite the 'assembly_names' dictionary \n",
    "## These .txt files have the clusters in order and the contig info as a tsv\n",
    "for ass, dirct in assembly_names.items():\n",
    "    assembly_names[ass] = pd.read_csv(dirct+'geneclusters.txt', sep = '\\t', header=None)\n",
    "\n",
    "\n",
    "\n",
    "def get_contig(cluster_name_string):\n",
    "    '''Feed this funct a cluster and it will fetch the contig it came from'''\n",
    "    \n",
    "    #get the assembly of origin form cluster name to select the right dataframe form thr dict\n",
    "    name_string_split = cluster_name_string.split('_')\n",
    "    assembly_name = name_string_split[0] # This will get us the dataframe form the dictonary\n",
    "    \n",
    "    ## Get the cluster number to query the dataframe by index\n",
    "    ## This needs to be an interger...\n",
    "    clust_number = name_string_split[2].replace('.fna','').replace('cluster','').lstrip('0')\n",
    "    clust_index = (int(clust_number)-1)\n",
    "    \n",
    "    \n",
    "    contig = assembly_names[assembly_name].iloc[clust_index][1]\n",
    "    \n",
    "    return contig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the contig info to the dataframe of BGC\n",
    "clus_df['contig'] = clus_df['genome'].apply(lambda cluster: get_contig(cluster)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From the contig get bin or unbinned \n",
    "\n",
    "##Make a dictionary of contigs for each bin\n",
    "\n",
    "bins_contigs={} # dictionary of contigs for each bin\n",
    "for each_bin in os.listdir(bins_dir):\n",
    "    bins_seqs = SeqIO.parse(bins_dir+each_bin, 'fasta')\n",
    "    \n",
    "    bins_contigs[each_bin]= [contig.id for contig in bins_seqs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## funct to get bin from contig by querying the 'bins_contigs' dict\n",
    "\n",
    "def contig_to_bin(contig):\n",
    "    for k, v in bins_contigs.items():\n",
    "        if contig in v:\n",
    "            return k\n",
    "        \n",
    "    return 'Unbinned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the binned info to the dataframe of BGC\n",
    "clus_df['binned'] = clus_df['contig'].apply(lambda contig: contig_to_bin(contig)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select the representitive BGC. If it was binned it is the rep for that secondary_cluster; else the longest one is chosen\n",
    "\n",
    "\n",
    "## Group the dataframe by sec_clust then check for a binned cluster\n",
    "binned_BGCs_list = []\n",
    "for sec_clus, df in clus_df.groupby(['secondary_cluster'],sort=False):\n",
    "    if any(df[df['binned']!='Unbinned']):\n",
    "        df_bins = df[df['binned']!='Unbinned']\n",
    "        \n",
    "        #if len(df_bins)>1: ## if there are redundant BGC sequences in the bins, get the longest (this only happens about 5 times)\n",
    "        #    df_bins = df_bins[df_bins['length']==df_bins['length'].max()].drop_duplicates(subset ='secondary_cluster') ## drop dupes to kill any clusters of the same lenght\n",
    "            \n",
    "    binned_BGCs_list.append(df_bins)\n",
    "           \n",
    "binned_BGCs_df = pd.concat(binned_BGCs_list)\n",
    "\n",
    "## manually remove the few redundant clusters with out removing the intra-bin cluster repetes \n",
    "binned_BGCs_df = binned_BGCs_df[~binned_BGCs_df['genome'].isin(['MH-Pat-all_Masurca_cluster041.fna', 'MH-s5_L2_cluster052.fna'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This can check for duplicate\n",
    "\n",
    "# for sc, df in binned_BGCs_df.groupby('secondary_cluster'):\n",
    "#     if len(df) >1:\n",
    "#         print(sc)\n",
    "#         print(df[['genome', 'secondary_cluster','length']])\n",
    "#         print('')\n",
    "#         print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deal with the clusters that are not represented in a bin.\n",
    "\n",
    "## filter out the sec_clusters that are binned to get all unbinned BGCs\n",
    "unbinned_BGC_df = clus_df[~clus_df['secondary_cluster'].isin(binned_BGCs_df['secondary_cluster'])]\n",
    "\n",
    "##Select the unique longest BGC for each group  \n",
    "long_Unique_BGCs = unbinned_BGC_df.groupby(['secondary_cluster'],sort=False).apply(lambda g: g[g.length==g.length.max()]).drop_duplicates(subset ='secondary_cluster')['genome'] \n",
    "\n",
    "## Reset the unbinned_df to the selected BGCs\n",
    "unbinned_BGC_df = unbinned_BGC_df[unbinned_BGC_df['genome'].isin(long_Unique_BGCs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_clusters = pd.concat([binned_BGCs_df,unbinned_BGC_df]) #Stack the binned and unbinned dfs together to make final\n",
    "rep_clusters['Cluster number'] = np.arange(1, len(rep_clusters) + 1) ## This will generate the cluster number for the final antismash output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the cluster type to the df\n",
    "\n",
    "def fna_to_GBK(fasta):\n",
    "    \n",
    "    asmby_name_map = {'MH-Pat-all':Pat_all_AS,\n",
    "                  'MH-Pat-sr':Pat_sr_AS,\n",
    "                'MH-s1':S1_AS,\n",
    "                'MH-s2':S2_AS,\n",
    "                'MH-s3':S3_AS,\n",
    "                'MH-s5':S5_AS}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    fasta_split = fasta.split('_')\n",
    "    assembly_name = fasta_split[0]\n",
    "    cluster_name = fasta_split[-1].replace('.fna', '')\n",
    "    \n",
    "    gbk_list = [gbk for gbk in os.listdir(asmby_name_map[assembly_name]) if 'gbk' in gbk and 'final' not in gbk]\n",
    "    \n",
    "    for gbk in gbk_list:\n",
    "        if gbk.split('.')[-2] == cluster_name:\n",
    "            return os.path.join(asmby_name_map[assembly_name],gbk)\n",
    "\n",
    "\n",
    "\n",
    "#funct to query the gbk for the cluster type\n",
    "\n",
    "def get_bgc_type(bin_name):\n",
    "    \n",
    "    gbk_path = fna_to_GBK(bin_name)\n",
    " \n",
    "    gbk = SeqIO.parse(gbk_path, 'genbank')\n",
    "    for rec in gbk:\n",
    "        return rec.features[0].qualifiers['product'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n"
     ]
    }
   ],
   "source": [
    "a_gbk_path = 'data/AS_data/MH-Pat-all_Masurca.AS/c00001_scf7180...cluster003.gbk'\n",
    "\n",
    "b_gbk_path = 'data/AS_data/MH-s3_L2.2k.fa.AS/c00001_NODE_1_...cluster083.gbk'\n",
    "\n",
    "gbk = SeqIO.parse(b_gbk_path, 'genbank')\n",
    "\n",
    "for rec in gbk:\n",
    "    if rec.features[1].type == 'cluster':\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matt/Desktop/BGC_Cluter_dRep'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Write to table \n",
    "## Not all of the clusters in this output can be 're-antismashed'. This is fixed up in the 'Unique_antismashy notebook'\n",
    "\n",
    "rep_clusters.drop(['primary_cluster','path'],axis=1).to_csv('Unique_BGC_tab.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matt/Desktop/BGC_Cluter_dRep/data/BGCs_fastas/MH-s3/MH-s3_L2_cluster083.fna'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_clusters.iloc[0]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Legacy code...\n",
    "## get the longest BGC for as the representitve \n",
    "#rep_clusters = clus_df.groupby(['secondary_cluster'],sort=False).apply(lambda g: g[g.length==g.length.max()]).drop_duplicates(subset ='secondary_cluster') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make a mulitfasta file with all the rep BGCs\n",
    "\n",
    "rep_cluster_seq_objs = [SeqIO.parse(path, 'fasta') for path in rep_clusters['path']]\n",
    "records = []\n",
    "for seq_object in rep_cluster_seq_objs:\n",
    "    for record in seq_object:\n",
    "        records.append(record)\n",
    "\n",
    "        \n",
    "SeqIO.write(records,'assemblies_rep_BGC.fasta', 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the BGCs from the ./final_bins_AS results dir - like befor\n",
    "\n",
    "# data directory \n",
    "final_AS_dir = data_dir + '/final_bins_AS/'\n",
    "\n",
    "\n",
    "# get the GBKs \n",
    "final_gbks = []\n",
    "for AS_DIR in os.listdir(final_AS_dir):\n",
    "    for file in os.listdir(final_AS_dir+AS_DIR):\n",
    "        if file.split('.')[-1] == 'gbk':\n",
    "            if file.split('.')[-2] != 'final':\n",
    "                final_gbks.append(final_AS_dir+AS_DIR+'/'+ file)\n",
    "\n",
    "                \n",
    "# Make a directroy to put them in\n",
    "if not os.path.isdir('data/final_bins_BGCs_fastas'):\n",
    "    os.makedirs('data/final_bins_BGCs_fastas')\n",
    "    \n",
    "# Write a fasta file from each GBK and stuff it into the dir that was just made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_gbks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_records = []\n",
    "\n",
    "for gbk in final_gbks:\n",
    "\n",
    "    ## Generate a unique header for the upcoming seq record\n",
    "    gbk_name_split = gbk.split('/') \n",
    "    assem_name = gbk_name_split[3]\n",
    "    clust_name = gbk_name_split[4].split('.')[-2]\n",
    "\n",
    "    header = assem_name + '_' + clust_name\n",
    "    #print(header)\n",
    "\n",
    "    ## Get seq record for the BGC\n",
    "    records = SeqIO.parse(gbk, \"genbank\")\n",
    "    for rec in records:\n",
    "        rec.id = header ## changer the header for the record to the one generated above\n",
    "        seq_records.append(rec)\n",
    "        \n",
    "        \n",
    "#Write the seq_records to fastas in a dir \n",
    "\n",
    "## Write out the results to a named directory inside the output directory\n",
    "for rec in seq_records:\n",
    "    id = rec.id\n",
    "    seq = rec.seq\n",
    "    id_file = open('data/final_bins_BGCs_fastas'+'/'+id+'.fna', \"w\")\n",
    "    id_file.write(\">\"+str(id)+\"\\n\"+str(seq))\n",
    "    id_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dRep the final_bins BGCs with the assemblies dReped BGCs\n",
    "# make the list of fasta file to dREP\n",
    "\n",
    "\n",
    "final_bins_BGC = os.listdir('data/final_bins_BGCs_fastas/')\n",
    "final_bins_BGC_paths = ['data/final_bins_BGCs_fastas/'+ bcg for bcg in final_bins_BGC]\n",
    "assemblies_BGC_dRepd_paths = list(rep_clusters['path'])\n",
    "\n",
    "fnal_ass_combind = final_bins_BGC_paths + assemblies_BGC_dRepd_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the clustering wrapper \n",
    "\n",
    "#drep.d_cluster.d_cluster_wrapper('./combind_drep_wd_gANI_25', P_ani = 0.95, S_ani =.995, S_algorithm = 'ANImf', genomes = fnal_ass_combind, COV_THRESH = 0.05)\n",
    "#drep.d_analyze.d_analyze_wrapper('./combind_drep_wd_gANI_25', plots = ['1','2','3','4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the list of BGCs that dont cluster with one of the final_bin derive BGCs\n",
    "comb_clus_df = pd.read_csv('combind_drep_wd_gANI_25/data_tables/Cdb.csv')\n",
    "g_comb_clus_df = comb_clus_df.groupby('secondary_cluster', sort=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of unbinned gene cluster!\n",
    "unbinned_BCGs = []\n",
    "\n",
    "for sc, df in g_comb_clus_df:\n",
    "    if all(~df['genome'].isin(final_bins_BGC)):\n",
    "        unbinned_BCGs = unbinned_BCGs + list(df['genome'])\n",
    "              \n",
    "        \n",
    "binned_BCGs = []\n",
    "\n",
    "for sc, df in g_comb_clus_df:\n",
    "    if any(df['genome'].isin(final_bins_BGC)):\n",
    "        binned_BCGs.append(list(df['genome']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = unbinned_BCGs.set_index(['secondary_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cp the unbinnded BGCs GBKs to a results file and do a rename\n",
    "#make a resutes dir\n",
    "if not os.path.isdir('Unbinned_GBKs'):\n",
    "    os.makedirs('Unbinned_GBKs')\n",
    "\n",
    "for fasta in unbinned_BCGs:\n",
    "    file_tup = fna_to_GBK(fasta)\n",
    "    copyfile(file_tup[0], 'Unbinned_GBKs/'+file_tup[1])\n",
    "    \n",
    "## write the list of paths to a file\n",
    "\n",
    "with open('Unbinned_gbks.txt', 'w') as f:\n",
    "    for item in sorted(os.listdir('Unbinned_GBKs')):\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('Unbinned_FNAs'):\n",
    "    os.makedirs('Unbinned_FNAs')\n",
    "    \n",
    "for x,y in product(unbinned_BCGs, fnas_list):\n",
    "    if x == y.split('/')[-1]:\n",
    "        copyfile(y, 'Unbinned_FNAs/'+x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
